import os
import subprocess
import requests
import json

LOG_FILE = "execution_log.txt"
OLLAMA_MODEL = "gemma2:9b"

def get_latest_log():
    try:
        if not os.path.exists(LOG_FILE):
            return "ë¡œê·¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        with open(LOG_FILE, "r") as f:
            content = f.read()
            # ë§ˆì§€ë§‰ ì‹¤í–‰ ì„¸ì…˜ë§Œ ì¶”ì¶œ (--- ì‹¤í–‰ ì‹œê° ê¸°ì¤€)
            sessions = content.split("--- ì‹¤í–‰ ì‹œê°")
            if len(sessions) > 1:
                return sessions[-1]
            return content
    except Exception as e:
        return f"ë¡œê·¸ ì½ê¸° ì‹¤íŒ¨: {str(e)}"

def summarize_with_ollama(text):
    print(f"ğŸ¤– Ollama ({OLLAMA_MODEL})ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ ìš”ì•½ ì¤‘...")
    
    url = "http://localhost:11434/api/generate"
    prompt = f"""
    ì•„ë˜ëŠ” AI íˆ¬ì ì‹œìŠ¤í…œì˜ ì‹¤í–‰ ë¡œê·¸ì…ë‹ˆë‹¤. 
    ì´ ì¤‘ì—ì„œ ê°œë°œìê°€ ì•Œì•„ì•¼ í•  í•µì‹¬ ì—ëŸ¬ë‚˜ ê²°ê³¼ë§Œ 500ì ì´ë‚´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
    ì˜ì–´ë¡œ ëœ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´ í•µì‹¬ ì˜ë¯¸ë¥¼ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

    ë¡œê·¸ ë‚´ìš©:
    {text}
    """
    
    data = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False
    }
    
    try:
        response = requests.post(url, json=data)
        if response.status_code == 200:
            return response.json().get("response", "ìš”ì•½ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            return f"Ollama ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}"
    except Exception as e:
        return f"Ollama ì—°ê²° ì‹¤íŒ¨: {str(e)}"

if __name__ == "__main__":
    log_text = get_latest_log()
    summary = summarize_with_ollama(log_text)
    
    print("\n" + "="*50)
    print("ğŸ“ ë¡œê·¸ ìš”ì•½ ê²°ê³¼ (Julesì—ê²Œ ì „ë‹¬ìš©)")
    print("="*50)
    print(summary)
    print("="*50)
